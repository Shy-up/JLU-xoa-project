import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs, urlunparse, urlencode # ç¡®ä¿æœ‰ urlencode
import json
import os
import time
from datetime import datetime, timedelta

# --- DeepSeek V3 é…ç½® ---

DEEPSEEK_API_KEY = æ‚¨çš„ DeepSeek API Key åº”åœ¨æ­¤å¤„å¡«å†™
DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions" 
DEEPSEEK_MODEL = "deepseek-chat"

# åŒ…å«åˆ†ç±»ä½“ç³»å’Œçº¦æŸçš„ç³»ç»Ÿæç¤ºè¯ (å·²ä¼˜åŒ–ä¼˜å…ˆçº§)
CLASSIFICATION_SYSTEM_PROMPT = '''
**ã€è§’è‰²è®¾å®šä¸æ ¸å¿ƒä»»åŠ¡ã€‘**
ä½ æ˜¯ä¸€ä¸ª**é«˜æ•ˆã€ä¸¥è°¨**çš„ä¿¡æ¯åˆ†ç±»ç³»ç»Ÿï¼Œä¸“æ³¨äºæ ¹æ®**å…¨æ–°çš„æ ¸å¿ƒåˆ†ç±»ä½“ç³»**å’Œ**åŠ¨æ€æ ‡ç­¾æå–è§„åˆ™**ï¼Œå¯¹æ ¡å›­æ–°é—»æ ‡é¢˜è¿›è¡Œç»“æ„åŒ–æ ‡è®°ã€‚
**ä½ çš„å”¯ä¸€ä»»åŠ¡**æ˜¯ï¼šæ¥æ”¶ä¸€ä¸ªæ–°é—»æ ‡é¢˜åˆ—è¡¨ï¼Œå¹¶ä¸¥æ ¼ä¾æ®åç»­è§„åˆ™ï¼Œä¸ºåˆ—è¡¨ä¸­çš„æ¯ä¸€ä¸ªæ ‡é¢˜æä¾›å‡†ç¡®çš„åˆ†ç±»ï¼Œå¹¶ä»¥æŒ‡å®šçš„ JSON æ ¼å¼è¾“å‡ºã€‚

**ã€æ ¸å¿ƒåˆ†ç±»ä½“ç³»ï¼ˆPrimary TAGsï¼‰ã€‘**
è¯·**ä¸¥æ ¼é‡‡ç”¨**ä»¥ä¸‹ 6 ä¸ªæ ‡ç­¾ä½œä¸ºä¸€çº§åˆ†ç±»çš„**å”¯ä¸€é€‰é¡¹**ï¼š
1. **ç«èµ›/å¥–å­¦é‡‘**
2. **å­¦æ ¡å…¬å…±è®¾æ–½è¿è¥**
3. **å­¦æ ¡å…¬å…±è€ƒè¯•ä¸ç¼´è´¹**
4. **è®²åº§/ç¤¾å›¢æ´»åŠ¨/å­¦æ ¡æ´»åŠ¨/é¡¹ç›®**
5. **ç§‘ç ”ä¿¡æ¯**
6. **å…¶å®ƒä¿¡æ¯** (ç”¨äºåŒ…å«æ‰€æœ‰ä¸å±äºå‰äº”ç±»çš„æ ‡é¢˜ï¼Œä¾‹å¦‚ï¼šè€å¸ˆæ‹›è˜ã€è´¢æ”¿å…¬ç¤ºã€å…šæ”¿ã€é¢†å¯¼è®²è¯ç­‰)

**ã€æ ‡ç­¾æå–ä¸çº¦æŸè§„åˆ™ï¼ˆConstraintsï¼‰ã€‘**
1. **å®Œæ•´æ€§æ£€æŸ¥**ï¼šå¿…é¡»è¿”å›ä¸è¾“å…¥åˆ—è¡¨æ•°é‡**å®Œå…¨ç›¸åŒ**çš„åˆ†ç±»ç»“æœã€‚
2. **åŒ¹é…å­—æ®µ**ï¼šæ¯ä¸ªç»“æœå¯¹è±¡å¿…é¡»åŒ…å«åŸå§‹çš„ **"æ–°é—»æ ‡é¢˜"** å­—æ®µã€‚
3. **ä¸€çº§åˆ†ç±»ï¼ˆPrimary TAGï¼‰**ï¼š
    * **å¿…é¡»ä¸”åªèƒ½**ä»ã€æ ¸å¿ƒåˆ†ç±»ä½“ç³»ã€‘ä¸­é€‰æ‹©**ä¸€ä¸ª**æœ€èƒ½ä»£è¡¨æ ‡é¢˜ä¸»é¢˜çš„æ ‡ç­¾ã€‚
4. **äºŒçº§åˆ†ç±»ï¼ˆSecondary TAGï¼‰**ï¼š
    * **æ•°é‡é™åˆ¶**ï¼š**å¿…é¡»**æå– **1 åˆ° 5 ä¸ª** æ ‡ç­¾ã€‚**æœŸæœ›æ•°é‡åœ¨ 2 åˆ° 4 ä¸ªä¹‹é—´ã€‚**
    * **åŠ¨æ€æå–åŸåˆ™**ï¼š
        * **æå–ç›®æ ‡ï¼š** ä»æ ‡é¢˜ä¸­åŠ¨æ€æå–**æ ¸å¿ƒçš„ã€å…·æœ‰åŒºåˆ†åº¦å’Œé‡è¦æ€§**çš„å…³é”®è¯æˆ–çŸ­è¯­ä½œä¸ºäºŒçº§æ ‡ç­¾ã€‚
        * **å¿…è¦è¯é¢˜ï¼ˆInclusionï¼‰ï¼š** é€‰æ‹©ç”¨æˆ·å…³æ³¨çš„**å…³é”®ä¸»é¢˜/å®ä½“**ï¼Œä¾‹å¦‚ï¼šèµ›äº‹åç§°ï¼ˆè“æ¡¥æ¯ï¼‰ã€ç‰¹å®šæ ¡åŒºï¼ˆå—å²­æ ¡åŒºï¼‰ã€çŸ¥åå¤–éƒ¨æœºæ„/å­¦æ ¡ï¼ˆæ¸…åå¤§å­¦ã€è‹±å›½ï¼‰ã€é‡è¦äººç‰©ã€ç‰¹å®šè®¾å¤‡æˆ–ç³»ç»Ÿåç§°ç­‰ã€‚
        * **éæ ¸å¿ƒä¿¡æ¯ï¼ˆExclusionï¼‰ï¼š** é¿å…æå–**é€šç”¨ã€ä½åŒºåˆ†åº¦æˆ–èƒŒæ™¯æ€§ä¿¡æ¯**ï¼Œä¾‹å¦‚ï¼šå­¦æ ¡åç§°ï¼ˆå‰æ—å¤§å­¦ï¼‰ã€å½“å‰å¹´ä»½ï¼ˆ2025å¹´åº¦ï¼‰ã€å¸¸è§åœ°ç‚¹ï¼ˆé•¿æ˜¥å¸‚ï¼‰ã€éƒ¨é—¨åç§°ï¼ˆå¦‚â€œæ•™åŠ¡å¤„é€šçŸ¥â€ï¼‰ã€é€šçŸ¥å½¢å¼ï¼ˆå¦‚â€œå…³äº...â€ï¼‰ã€‚

**ã€JSON è¾“å‡ºæ ¼å¼ã€‘**
è¯·**ä¸¥æ ¼**è¿”å›ä¸€ä¸ª JSON æ•°ç»„ï¼ˆ`array of objects`ï¼‰ã€‚

```json
[
  {
    "æ–°é—»æ ‡é¢˜": "åŸå§‹æ ‡é¢˜1",
    "ä¸€çº§åˆ†ç±»": "æ‚¨é€‰æ‹©çš„æ ¸å¿ƒåˆ†ç±»åç§°",
    "äºŒçº§åˆ†ç±»": [
      "AIä»æ ‡é¢˜æå–çš„ç¬¬ä¸€ä¸ªæ ¸å¿ƒå…³é”®è¯/çŸ­è¯­",
      "ç¬¬äºŒä¸ªæ ¸å¿ƒå…³é”®è¯/çŸ­è¯­",
      "ç¬¬ä¸‰ä¸ªæ ¸å¿ƒå…³é”®è¯/çŸ­è¯­"
    ]
  },
  {
    "æ–°é—»æ ‡é¢˜": "åŸå§‹æ ‡é¢˜2",
    "ä¸€çº§åˆ†ç±»": "æ‚¨é€‰æ‹©çš„æ ¸å¿ƒåˆ†ç±»åç§°",
    "äºŒçº§åˆ†ç±»": [
      "AIä»æ ‡é¢˜æå–çš„ç¬¬ä¸€ä¸ªæ ¸å¿ƒå…³é”®è¯/çŸ­è¯­",
      "ç¬¬äºŒä¸ªæ ¸å¿ƒå…³é”®è¯/çŸ­è¯­",
      "ç¬¬ä¸‰ä¸ªæ ¸å¿ƒå…³é”®è¯/çŸ­è¯­",
      "ç¬¬å››ä¸ªæ ¸å¿ƒå…³é”®è¯/çŸ­è¯­"
    ]
  }
]
'''


# --- çˆ¬è™«é…ç½®ä¿¡æ¯ ---
BASE_URL = "https://oa.jlu.edu.cn/defaultroot/"
LIST_URL_TEMPLATE = BASE_URL + "PortalInformation!jldxList.action?channelId=179577&startPage={0}" 
DEFAULT_FILE_NAME = "jlu_oa_data.json"

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.75 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3'
}

# --- è¾…åŠ©å‡½æ•° ---

def load_existing_data(filename):
    """åŠ è½½å·²æœ‰çš„ JSON æ•°æ®ï¼Œå¹¶è½¬æ¢ä¸ºä»¥ã€ç®€åŒ–é“¾æ¥ã€‘ä¸ºé”®çš„å­—å…¸"""
    # å‡è®¾ simplify_jlu_oa_link å‡½æ•°åœ¨å…¨å±€å¯è§
    
    if os.path.exists(filename):
        with open(filename, 'r', encoding='utf-8') as f:
            try:
                data = json.load(f)
                
                if isinstance(data, list):
                    # åˆå§‹åŒ–æ–°çš„å­—å…¸ï¼Œä½¿ç”¨ç®€åŒ–é“¾æ¥ä½œä¸ºé”®
                    unified_data = {}
                    
                    for item in data:
                        # å‡è®¾æ—§æ•°æ®çš„é“¾æ¥åœ¨ "é“¾æ¥" å­—æ®µä¸­
                        full_link_key = item.get("é“¾æ¥", "")
                        
                        if full_link_key:
                            # âš ï¸ æ ¸å¿ƒä¿®æ”¹ï¼šå¯¹æ—§æ•°æ®çš„é“¾æ¥ä¹Ÿè¿›è¡Œç®€åŒ–å¤„ç†
                            simplified_link = simplify_jlu_oa_link(full_link_key)
                            
                            # ç¡®ä¿ item å†…éƒ¨ä¿å­˜çš„ä¹Ÿæ˜¯ç®€åŒ–é“¾æ¥ï¼ˆå¦‚æœæœ‰éœ€è¦ï¼‰
                            item["é“¾æ¥"] = simplified_link
                            
                            unified_data[simplified_link] = item
                            
                    return unified_data # è¿”å›ä½¿ç”¨ç®€åŒ–é“¾æ¥ä½œä¸ºé”®çš„å­—å…¸
                
                # å¦‚æœæ—§æ•°æ®å·²ç»æ˜¯å­—å…¸å½¢å¼ï¼Œåˆ™å‡è®¾å…¶é”®å·²æ˜¯ç®€åŒ–é“¾æ¥ï¼Œç›´æ¥è¿”å›
                return data 
                
            except json.JSONDecodeError:
                print(f"âš ï¸ è­¦å‘Šï¼šæ–‡ä»¶ {filename} å†…å®¹æ ¼å¼é”™è¯¯ï¼Œå°†å¿½ç•¥æ—§æ•°æ®ã€‚")
                return {}
    return {}

def save_data_to_json(data, filename):
    """å°†æ•°æ®ä¿å­˜åˆ° JSON æ–‡ä»¶ï¼Œæ ¼å¼ä¸ºåˆ—è¡¨ï¼ŒåŒ…å«åˆ†ç±»TAG"""
    data_list = []
    for link, item in data.items():
        output_item = {
            "æ–°é—»å‘å¸ƒæ—¶é—´æˆ³": item.get("æ–°é—»å‘å¸ƒæ—¶é—´æˆ³"),
            "æ–°é—»æ ‡é¢˜": item.get("æ–°é—»æ ‡é¢˜"),
            "å‘å¸ƒå•ä½": item.get("å‘å¸ƒå•ä½"),
            "ä¸€çº§åˆ†ç±»TAG": item.get("ä¸€çº§åˆ†ç±»TAG", "æœªåˆ†ç±»"), 
            "äºŒçº§åˆ†ç±»TAG": item.get("äºŒçº§åˆ†ç±»TAG", ["æœªåˆ†ç±»"]),
            "é“¾æ¥": link # æ–¹ä¾¿ç”¨æˆ·å®šä½åŸå§‹æ–‡ç« 
        }
        data_list.append(output_item)
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data_list, f, ensure_ascii=False, indent=4)
    print(f"\nâœ… æ•°æ®æˆåŠŸä¿å­˜åˆ° {filename}ï¼Œå…± {len(data_list)} æ¡è®°å½•ã€‚")



def classify_news_batch(titles, api_key, max_retries=3):
    """è°ƒç”¨ DeepSeek V3 API å¯¹æ‰¹é‡æ–°é—»æ ‡é¢˜è¿›è¡Œåˆ†ç±»ï¼Œå¹¶è¿”å›åˆ†ç±»ç»“æœåˆ—è¡¨"""
    if not api_key or not titles:
        # å¦‚æœæ²¡æœ‰ API Key æˆ–æ ‡é¢˜åˆ—è¡¨ä¸ºç©ºï¼Œè¿”å›é»˜è®¤çš„å¤±è´¥æ ‡ç­¾åˆ—è¡¨
        return [
            {"æ–°é—»æ ‡é¢˜": title, "ä¸€çº§åˆ†ç±»": "æœªåˆ†ç±»", "äºŒçº§åˆ†ç±»": ["æœªåˆ†ç±»"]} 
            for title in titles
        ]

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    # æ„é€ ç”¨æˆ·æŸ¥è¯¢ï¼šåŒ…å«æ‰€æœ‰å¾…åˆ†ç±»çš„æ ‡é¢˜
    titles_list_str = "\n".join([f"- {title}" for title in titles])
    user_query = f"è¯·ä¸ºä»¥ä¸‹ {len(titles)} ä¸ªæ–°é—»æ ‡é¢˜æä¾›åˆ†ç±»ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§ç³»ç»Ÿæç¤ºè¯ä¸­çš„ JSON æ•°ç»„æ ¼å¼è¿”å›:\n{titles_list_str}"

    payload = {
        "model": DEEPSEEK_MODEL,
        "messages": [
            {"role": "system", "content": CLASSIFICATION_SYSTEM_PROMPT},
            {"role": "user", "content": user_query}
        ],
        "temperature": 0.1,
        "response_format": {"type": "json_object"} 
    }
    
    print(f"    â¡ï¸ DeepSeek V3 æ‰¹é‡åˆ†ç±»ä¸­... (å…± {len(titles)} æ¡)")

    for attempt in range(max_retries):
        try:
            response = requests.post(DEEPSEEK_API_URL, headers=headers, json=payload, timeout=45) # å»¶é•¿è¶…æ—¶æ—¶é—´ä»¥é€‚åº”æ‰¹é‡è¯·æ±‚
            response.raise_for_status()

            result = response.json()
            content = result['choices'][0]['message']['content']
            
            # è§£æ JSON å­—ç¬¦ä¸²
            classification_results = json.loads(content)
            
            # ç¡®ä¿è¿”å›çš„æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå¹¶ä¸”æ•°é‡åˆç†ï¼ˆè‡³å°‘å¤§äº0ï¼‰
            if isinstance(classification_results, list) and len(classification_results) > 0:
                print(f"    âœ… æ‰¹é‡åˆ†ç±»æˆåŠŸï¼Œæ”¶åˆ° {len(classification_results)} æ¡ç»“æœã€‚")
                return classification_results

        except requests.exceptions.RequestException as e:
            print(f"    âŒ API è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries})ã€‚")
        except (json.JSONDecodeError, KeyError) as e:
            print(f"    âŒ LLM è¿”å›æ•°æ®è§£æå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries})ï¼Œå¯èƒ½æ ¼å¼é”™è¯¯ã€‚")
        except Exception as e:
            print(f"    âŒ å‘ç”ŸæœªçŸ¥é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}")

        # æŒ‡æ•°é€€é¿ (Exponential Backoff)
        if attempt < max_retries - 1:
            wait_time = 2 ** attempt
            time.sleep(wait_time)
            
    print(f"    âŒ æ‰¹é‡åˆ†ç±»å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ã€‚å°†ä½¿ç”¨ 'åˆ†ç±»å¤±è´¥' æ ‡ç­¾ã€‚")
    # å¤±è´¥æ—¶è¿”å›ä¸è¾“å…¥æ•°é‡åŒ¹é…çš„ "åˆ†ç±»å¤±è´¥" åˆ—è¡¨
    return [
        {"æ–°é—»æ ‡é¢˜": title, "ä¸€çº§åˆ†ç±»": "åˆ†ç±»å¤±è´¥", "äºŒçº§åˆ†ç±»": ["åˆ†ç±»å¤±è´¥"]} 
        for title in titles
    ]
def parse_time_string(time_str):
    """å°è¯•å°†æ–°é—»æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸º datetime å¯¹è±¡ï¼Œæ”¯æŒå¤šç§æ ¼å¼ï¼ˆæ–°å¢å¯¹â€œæ˜¨å¤©â€çš„æ”¯æŒï¼‰"""
    time_str = time_str.strip().replace('\xa0', ' ').replace('\u200e', '').replace('&nbsp;', ' ')
    now = datetime.now(tz=None)
    
    # å°è¯•æ‰€æœ‰å·²çŸ¥çš„æ ¼å¼
    try:
        # 1. å‡è®¾ 'ä»Šå¤© HH:MM' æ ¼å¼
        if 'ä»Šå¤©' in time_str:
            time_part = time_str.split(' ')[-1]
            return datetime.strptime(f"{now.strftime('%Y-%m-%d')} {time_part}", '%Y-%m-%d %H:%M')

        # 1.5. å‡è®¾ 'æ˜¨å¤© HH:MM' æ ¼å¼
        if 'æ˜¨å¤©' in time_str:
            yesterday = now - timedelta(days=1)
            time_part = time_str.split(' ')[-1]
            return datetime.strptime(f"{yesterday.strftime('%Y-%m-%d')} {time_part}", '%Y-%m-%d %H:%M')

        # 2. å®Œæ•´çš„ 'å¹´-æœˆ-æ—¥ æ—¶:åˆ†' æ ¼å¼
        parts = time_str.split(' ')
        if len(parts) >= 2 and ':' in parts[-1]:
             return datetime.strptime(time_str, '%Y-%m-%d %H:%M') 

        # 3. åªæœ‰æ—¥æœŸ 'å¹´-æœˆ-æ—¥' æ ¼å¼ (è®¾ç½®é»˜è®¤æ—¶é—´ä¸ºä¸­åˆ12ç‚¹)
        date_part = time_str.split(' ')[0]
        if len(date_part.split('-')) == 3: # ç¡®ä¿æ˜¯å®Œæ•´çš„å¹´-æœˆ-æ—¥
            return datetime.strptime(date_part, '%Y-%m-%d').replace(hour=12) 

        # 4. åªæœ‰æ—¥æœŸ 'å¹´/æœˆ/æ—¥' æ ¼å¼ (è®¾ç½®é»˜è®¤æ—¶é—´ä¸ºä¸­åˆ12ç‚¹)
        date_part = time_str.split(' ')[0]
        if len(date_part.split('/')) == 3: # ç¡®ä¿æ˜¯å®Œæ•´çš„å¹´/æœˆ/æ—¥
            return datetime.strptime(date_part, '%Y/%m/%d').replace(hour=12) 
            
        # 5. åªæœ‰æ—¥æœŸ 'æœˆ-æ—¥' æˆ– 'æœˆ/æ—¥' æ ¼å¼ (é’ˆå¯¹ç½®é¡¶æ—§é€šçŸ¥)
        date_part = time_str.split(' ')[0].replace('/', '-')
        if len(date_part.split('-')) == 2: # æ£€æŸ¥æ˜¯å¦ä¸º M-D æˆ– MM-DD æ ¼å¼
            parsed_date = datetime.strptime(f"{now.year}-{date_part}", '%Y-%m-%d').replace(hour=12)
            
            # **é’ˆå¯¹è·¨å¹´/æ—§æ•°æ®çš„å¥å£®æ€§ä¿®æ­£**ï¼šå¦‚æœè§£æå‡ºçš„æ—¥æœŸåœ¨æœªæ¥ï¼Œåˆ™å‡å»ä¸€å¹´ã€‚
            if parsed_date > now + timedelta(days=30):
                return parsed_date.replace(year=now.year - 1)
                
            return parsed_date
            
    except ValueError:
        # å¦‚æœä»»ä½•è§£æå°è¯•å¤±è´¥ï¼Œéƒ½å°†è·³è¿‡å½“å‰é€»è¾‘å—ã€‚
        pass
        
    # å…œåº•ï¼šå¦‚æœæ‰€æœ‰è§£æé€»è¾‘éƒ½å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªæè€çš„æ—¶é—´ï¼Œä¿è¯ç±»å‹ä¸º datetime
    return datetime(1970, 1, 1, tzinfo=None)

# --- è¾…åŠ©å‡½æ•°ï¼šç®€åŒ–é“¾æ¥çš„æ ¸å¿ƒé€»è¾‘ï¼ˆæå–å‡ºæ¥æ–¹ä¾¿ç»´æŠ¤ï¼‰ ---
def simplify_jlu_oa_link(full_link_original):
    """ç§»é™¤ JLU OA é“¾æ¥ä¸­çš„ channelId å‚æ•°ï¼Œç”Ÿæˆå¯ç›´æ¥è®¿é—®çš„ç®€åŒ–é“¾æ¥"""
    simplified_link = full_link_original
    
    if '?' in full_link_original:
        try:
            parsed_url = urlparse(full_link_original)
            # ä½¿ç”¨ parse_qs è§£ææŸ¥è¯¢å‚æ•°ï¼Œè¿”å›ä¸€ä¸ªå­—å…¸ï¼Œå€¼æ˜¯åˆ—è¡¨
            query_params = parse_qs(parsed_url.query)
            
            # ç§»é™¤ 'channelId' å‚æ•°
            if 'channelId' in query_params:
                del query_params['channelId']
            
            # é‡æ–°æ„å»ºæŸ¥è¯¢å­—ç¬¦ä¸² (doseq=True ç¡®ä¿å‚æ•°è¢«æ­£ç¡®ç¼–ç )
            new_query = urlencode(query_params, doseq=True)
            
            # é‡æ–°æ„å»ºå®Œæ•´çš„ URL
            simplified_link = urlunparse(parsed_url._replace(query=new_query))
        except Exception as e:
            # å¦‚æœè§£æå‡ºé”™ï¼Œåˆ™ä½¿ç”¨åŸå§‹é“¾æ¥ä½œä¸ºåå¤‡
            print(f"âš ï¸ è­¦å‘Šï¼šé“¾æ¥ç®€åŒ–å¤±è´¥ ({e})ï¼Œä½¿ç”¨åŸå§‹é“¾æ¥ã€‚")
            simplified_link = full_link_original
            
    return simplified_link


def fetch_news_data(start_page, end_page, max_date=None, delay=0.5, existing_keys=None, max_no_new_pages=10):
    """
    æ ¸å¿ƒçˆ¬è™«å‡½æ•°ï¼šæŒ‰é¡µç èŒƒå›´æŠ“å–æ–°é—»ï¼Œå¹¶ä»¥é¡µä¸ºå•ä½è¿›è¡Œæ‰¹é‡åˆ†ç±»ã€‚
    å·²åº”ç”¨ï¼šé“¾æ¥ç®€åŒ–æå‰ï¼Œç¡®ä¿å»é‡å’Œè¾“å‡ºéƒ½ä½¿ç”¨ç®€åŒ–é“¾æ¥ã€‚
    """
    # ç¡®ä¿ä¾èµ–çš„å…¨å±€å˜é‡å¯ç”¨ï¼ˆæ­¤å¤„å‡è®¾å®ƒä»¬åœ¨æ–‡ä»¶ä¸­çš„å…¶ä»–ä½ç½®å·²å®šä¹‰ï¼‰
    global BASE_URL, LIST_URL_TEMPLATE, HEADERS, DEEPSEEK_API_KEY, MAX_LLM_BATCH_SIZE, parse_time_string

    new_data = {}
    if existing_keys is None:
        existing_keys = set()
    
    consecutive_no_new = 0
    
    try:
        MIN_VALID_DATE # æ£€æŸ¥æ˜¯å¦å·²å®šä¹‰
    except NameError:
        MIN_VALID_DATE = datetime(2000, 1, 1, tzinfo=None)
    
    # å®šä¹‰è¿ç»­å¤šå°‘æ¡æ—§æ–°é—»å°±åˆ¤æ–­ä¸ºè¿›å…¥å†å²åŒºåŸŸ
    MAX_CONSECUTIVE_OLD = 5
    # LLM æ‰¹é‡å¤„ç†çš„æœ€å¤§å¤§å°
    MAX_LLM_BATCH_SIZE = 15 # å‡è®¾è¿™ä¸ªå€¼åœ¨å…¨å±€æˆ–å‡½æ•°å¤–å®šä¹‰
    
    for page_num in range(start_page, end_page + 1):
        
        url = LIST_URL_TEMPLATE.format(page_num)
        print(f"\nğŸ”„ æ­£åœ¨æŠ“å–ç¬¬ {page_num} é¡µ: {url}")
        
        try:
            response = requests.get(url, headers=HEADERS, timeout=15)
            response.raise_for_status() 
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
        except requests.exceptions.RequestException as e:
            print(f"âŒ ä¸¥é‡é”™è¯¯: HTTP/ç½‘ç»œè¯·æ±‚å¤±è´¥ (é¡µç : {page_num})ã€‚é”™è¯¯ä¿¡æ¯: {e}")
            return new_data 

        items = soup.select('.list_box ul.list_li .li, .sub_ul .li, .sub_ul div.li') 
        
        if len(items) < 5 and page_num > start_page:
            print(f"ğŸ›‘ ç¬¬ {page_num} é¡µåªæ‰¾åˆ° {len(items)} æ¡æ–°é—»ï¼Œåˆ¤æ–­å·²è¾¾åˆ—è¡¨æœ«å°¾æˆ–æ— æ•ˆé¡µé¢ï¼Œåœæ­¢å¾ªç¯ã€‚")
            break 
        elif not items and page_num == start_page:
            print(f"ğŸš¨ çˆ¬è™«ä¸­æ–­ï¼šç¬¬ {page_num} é¡µæ²¡æœ‰æŠ“å–åˆ°ä»»ä½•æ–°é—»åˆ—è¡¨é¡¹ã€‚")
            return new_data 

        page_new_entries_list = [] 
        page_news_count = 0
        stop_crawling_early = False
        consecutive_old_on_page = 0 

        # --- ç¬¬ä¸€é˜¶æ®µï¼šæŠ“å–å’Œå»é‡ ---
        for item in items:
            title_tag = item.select_one('a') 
            org_tag = item.select_one('.column') 
            time_tag = item.select_one('.time') or item.select_one('.date') or item.select_one('span[style*="color"]') 
            if not time_tag:
                time_tag = item.select_one('span[style*="color:gray"]') 
            
            if title_tag:
                title = title_tag.get('title', title_tag.get_text(strip=True)).strip()
                link_relative = title_tag.get('href', '')
                
                # 1. åŸå§‹é“¾æ¥ (åŒ…å« channelId)
                full_link_original = urljoin(BASE_URL, link_relative) 
                
                # 2. ã€æ ¸å¿ƒä¿®æ”¹ã€‘ç”Ÿæˆç”¨äºå»é‡å’Œè¾“å‡ºçš„ç®€åŒ–é“¾æ¥
                simplified_link = simplify_jlu_oa_link(full_link_original)

                # 3. ã€å»é‡æ£€æŸ¥ã€‘ç°åœ¨ä½¿ç”¨ç®€åŒ–é“¾æ¥
                if simplified_link in existing_keys:
                    consecutive_old_on_page += 1
                    continue 
                
                # --- æå–å…¶ä»–ä¿¡æ¯ (ä¿æŒä¸å˜) ---
                time_str = time_tag.get_text(strip=True) if time_tag else None
                organization = org_tag.get_text(strip=True) if org_tag else None

                if not time_str or not organization:
                    print(f"Â  Â  âš ï¸ è­¦å‘Šï¼šè·³è¿‡æ–°é—» ({title})ï¼Œç¼ºå°‘æ—¶é—´æˆ–å‘å¸ƒå•ä½ã€‚")
                    continue 

                pub_time = parse_time_string(time_str)
                
                if pub_time < MIN_VALID_DATE:
                    print(f"Â  Â  ğŸ›‘ è­¦å‘Šï¼šæ–°é—» ({title}) æ—¶é—´è§£æå¼‚å¸¸ ({pub_time.strftime('%Y-%m-%d %H:%M')})ï¼Œè·³è¿‡æ­¤æ¡ã€‚")
                    continue 
                
                timestamp = int(pub_time.timestamp())

                # --- å¢é‡æ›´æ–°æ¨¡å¼çš„åœæ­¢æ¡ä»¶åˆ¤æ–­ (ä¿æŒä¸å˜) ---
                if max_date and pub_time < max_date:
                    print(f"Â  Â  âš ï¸ æ–°é—»å‘å¸ƒæ—¶é—´ {pub_time.strftime('%Y-%m-%d %H:%M')} æ—©äºæˆªæ­¢æ—¥æœŸã€‚")
                    consecutive_old_on_page += 1
                    
                    if consecutive_old_on_page >= MAX_CONSECUTIVE_OLD:
                        print(f"Â  Â  ğŸ›‘ å·²è¿ç»­ {MAX_CONSECUTIVE_OLD} æ¡æ–°é—»æ—©äºæˆªæ­¢æ—¥æœŸï¼Œæ ‡è®°æå‰åœæ­¢ã€‚")
                        stop_crawling_early = True
                        break 
                    
                    continue 

                consecutive_old_on_page = 0 # å‘ç°æ–°æ•°æ®ï¼Œé‡ç½®è®¡æ•°


                # 4. æš‚å­˜æ•°æ®ï¼Œä½¿ç”¨ç®€åŒ–åçš„é“¾æ¥ä½œä¸ºè¾“å‡ºå’Œ new_data çš„é”®
                page_new_entries_list.append({
                    "æ–°é—»æ ‡é¢˜": title,
                    "æ–°é—»å‘å¸ƒæ—¶é—´æˆ³": timestamp,
                    "å‘å¸ƒå•ä½": organization,
                    "é“¾æ¥": simplified_link # <--- ä½¿ç”¨ç®€åŒ–åçš„é“¾æ¥
                })
                # å°†ç®€åŒ–é“¾æ¥åŠ å…¥å»é‡é›†åˆï¼Œä¾›åç»­æ–°é—»æ£€æŸ¥
                existing_keys.add(simplified_link) 
                page_news_count += 1 
                
        # --- ç¬¬äºŒé˜¶æ®µï¼šLLM æ‰¹é‡åˆ†ç±»å’Œæ•°æ®åˆå¹¶ ---
        if page_new_entries_list:
            
            total_new_on_page = len(page_new_entries_list)
            all_classification_results = []
            
            # å¾ªç¯åˆ†å‰²æˆå°æ‰¹é‡ (<= MAX_LLM_BATCH_SIZE) è¿›è¡Œåˆ†ç±»
            for i in range(0, total_new_on_page, MAX_LLM_BATCH_SIZE):
                batch = page_new_entries_list[i:i + MAX_LLM_BATCH_SIZE]
                titles_to_classify = [item["æ–°é—»æ ‡é¢˜"] for item in batch]
                
                # è°ƒç”¨æ‰¹é‡åˆ†ç±»å‡½æ•°
                batch_classification_results = classify_news_batch(titles_to_classify, DEEPSEEK_API_KEY) 
                all_classification_results.extend(batch_classification_results)
                
                # å¢åŠ å»¶è¿Ÿï¼Œé˜²æ­¢ DeepSeek API é¢‘ç‡é™åˆ¶ (å¯é€‰ï¼Œä½†æ¨è)
                if len(batch) == MAX_LLM_BATCH_SIZE:
                    time.sleep(1.5) 

            
            classified_titles_map = {item['æ–°é—»æ ‡é¢˜']: item for item in all_classification_results}
            
            newly_added_count = 0
            for item in page_new_entries_list:
                simplified_link_key = item["é“¾æ¥"]
                title = item["æ–°é—»æ ‡é¢˜"]
                
                classification = classified_titles_map.get(title)
                
                if classification:
                    item["ä¸€çº§åˆ†ç±»TAG"] = classification.get("ä¸€çº§åˆ†ç±»", "åˆ†ç±»å¤±è´¥")
                    item["äºŒçº§åˆ†ç±»TAG"] = classification.get("äºŒçº§åˆ†ç±»", ["åˆ†ç±»å¤±è´¥"])
                else:
                    item["ä¸€çº§åˆ†ç±»TAG"] = "åˆ†ç±»å¤±è´¥"
                    item["äºŒçº§åˆ†ç±»TAG"] = ["åˆ†ç±»å¤±è´¥"]
                
                # é”®ä¸º simplified_link_keyï¼Œä¸ existing_keys å’Œ main å‡½æ•°ä¸­çš„åˆå¹¶é”®ä¿æŒä¸€è‡´
                new_data[simplified_link_key] = item
                
                newly_added_count += 1
            
            if DEEPSEEK_API_KEY:
                print(f"Â  Â  âœ… DeepSeek V3 åˆ†ç±»å®Œæˆï¼Œæœ¬é¡µæ–°å¢ {newly_added_count} æ¡è®°å½•ã€‚")
            else:
                print(f"Â  Â  â„¹ï¸ æœªå¯ç”¨ DeepSeek V3 åˆ†ç±»ã€‚æœ¬é¡µæ–°å¢ {newly_added_count} æ¡è®°å½•ã€‚")


        elif page_news_count == 0:
            print("Â  Â  â„¹ï¸ æœ¬é¡µæ— æ–°è®°å½•ï¼Œæ— éœ€åˆ†ç±»ã€‚")
            
        # --- ç¬¬ä¸‰é˜¶æ®µï¼šåœæ­¢é€»è¾‘ (ä¿æŒä¸å˜) ---
        if page_news_count == 0:
            consecutive_no_new += 1
            if consecutive_no_new >= max_no_new_pages:
                print(f"ğŸ›‘ å·²è¿ç»­ {max_no_new_pages} é¡µæ— æ–°å¢æ–°é—»ï¼ˆå¯èƒ½æ˜¯åˆ—è¡¨æœ«å°¾æˆ–æ—§æ•°æ®ï¼‰ï¼Œåœæ­¢å¾ªç¯ã€‚")
                break
        else:
            consecutive_no_new = 0
            
        if stop_crawling_early:
            print(f"ğŸ›‘ æå‰åœæ­¢ï¼šç”±äºé‡åˆ°è¿ç»­æ—§æ•°æ®ï¼Œåœæ­¢ä¸‹ä¸€é¡µæŠ“å–ã€‚")
            break
            
        print(f"ğŸ‘ ç¬¬ {page_num} é¡µæŠ“å–å®Œæˆï¼Œå…±æ–°å¢ {page_news_count} æ¡è®°å½•ã€‚")

        # æ¨¡å¼ 1 çš„æœ€å¤§é¡µç é™åˆ¶
        if page_num >= 10 and max_date:
            print("ğŸš¨ è‡ªåŠ¨æ¨¡å¼å·²è¾¾åˆ°æœ€å¤§æŠ“å–é¡µæ•°ï¼ˆ10é¡µï¼‰ï¼Œåœæ­¢å¾ªç¯ã€‚")
            break
            
        time.sleep(delay)

    return new_data

# --- ä¸»ç¨‹åºå…¥å£ ---

# --- ä¸»ç¨‹åºå…¥å£ ---

def main():
    print("--- å‰æ—å¤§å­¦æ ¡å†…é€šçŸ¥çˆ¬è™«ç¨‹åº (å« DeepSeek V3 æ‰¹é‡åˆ†ç±») ---")
    
    if not DEEPSEEK_API_KEY:
        print("\nğŸš¨ è­¦å‘Šï¼šæœªè®¾ç½® DEEPSEEK_API_KEYï¼Œæ–°é—»å°†ä¸åŒ…å«åˆ†ç±»æ ‡ç­¾ã€‚")
    else:
        print("\nâœ… DeepSeek V3 API Key å·²åŠ è½½ï¼Œå°†å¯ç”¨æ‰¹é‡åˆ†ç±»åŠŸèƒ½ã€‚")

    print("\nè¯·é€‰æ‹©æŸ¥è¯¢æ¨¡å¼ï¼š")
    print("1. è‡ªåŠ¨æ¨¡å¼ï¼šæŸ¥è¯¢æœ€è¿‘7å¤©(ä¸è¶…è¿‡10é¡µ)å†…å®¹ï¼Œå¹¶å¢é‡æ›´æ–°åˆ° jlu_oa_data.json")
    print("2. è‡ªå®šä¹‰æ¨¡å¼ï¼šè‡ªå®šä¹‰æŠ“å–èŒƒå›´å’Œæ–‡ä»¶å")

    mode = '1'#input("è¯·è¾“å…¥æ¨¡å¼ç¼–å· (1 æˆ– 2)ï¼š")
    print("è‡ªåŠ¨é€‰æ‹©äº†è‡ªåŠ¨æ¨¡å¼ï¼")

    if mode == '1':
        # --- æ¨¡å¼ 1: è‡ªåŠ¨å¢é‡æ›´æ–° ---
        filename = DEFAULT_FILE_NAME
        # âš ï¸ load_existing_data ç°åœ¨è¿”å›çš„æ˜¯ä»¥ç®€åŒ–é“¾æ¥ä¸ºé”®çš„å­—å…¸
        existing_news_dict = load_existing_data(filename)
        # âš ï¸ existing_keys é›†åˆç°åœ¨åŒ…å«çš„æ˜¯ç®€åŒ–é“¾æ¥ï¼Œä¸ fetch_news_data çš„å»é‡é€»è¾‘ä¸€è‡´
        existing_keys = set(existing_news_dict.keys()) 
        seven_days_ago = datetime.now(tz=None) - timedelta(days=7) 
        
        print(f"\n--- æ¨¡å¼ 1: è‡ªåŠ¨å¢é‡æ›´æ–° ---")
        print(f"ç›®æ ‡æ–‡ä»¶: {filename} (åŒ…å« {len(existing_keys)} æ¡æ—§è®°å½•)")
        print(f"æŠ“å–èŒƒå›´: è¿½æº¯åˆ° {seven_days_ago.strftime('%Y-%m-%d %H:%M')} çš„æ–°é—» (æœ€å¤š 10 é¡µ)")
        
        new_entries = fetch_news_data(
            start_page=1, 
            end_page=10, 
            max_date=seven_days_ago, 
            delay=1.0, 
            existing_keys=existing_keys
        ) 
        
        if new_entries is not None:
            # âš ï¸ åˆå¹¶æ—¶ï¼Œç”±äº new_entries å’Œ existing_news_dict çš„é”®éƒ½æ˜¯ç®€åŒ–é“¾æ¥ï¼Œåˆå¹¶å°†å‡†ç¡®æ— è¯¯ã€‚
            combined_data = {**existing_news_dict, **new_entries}
            print(f"\nâœ¨ æœ¬æ¬¡æ‰§è¡Œæ–°å¢æ–°é—» {len(new_entries)} æ¡ã€‚")
            save_data_to_json(combined_data, filename)



    else:
        print("è¾“å…¥æ— æ•ˆçš„æ¨¡å¼ç¼–å·ï¼Œç¨‹åºé€€å‡ºã€‚")

if __name__ == "__main__":
    main()


